{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283734de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36c9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import random\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "import copy\n",
    "from common import *\n",
    "from buildtrain import *\n",
    "from scipy.stats import rankdata\n",
    "from model import *\n",
    "import torch\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "from torch.nn.init import xavier_normal_\n",
    "from torch.nn import Parameter\n",
    "from ordered_set import OrderedSet\n",
    "from collections import defaultdict as ddict\n",
    "from data_loader import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346ed991",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63d7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self,data_path,data_name,strategy='one_to_n',batch_size=1000,test_batch_size=2000,device='gpu'):\n",
    "        self.data_path = data_path\n",
    "        self.data_name = data_name\n",
    "        self.strategy = strategy\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.ent2edges = None\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Reading in raw triples and converts it into a standard format.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self.p.dataset:         Takes in the name of the dataset (FB15k-237)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self.ent2id:            Entity to unique identifier mapping\n",
    "        self.id2rel:            Inverse mapping of self.ent2id\n",
    "        self.rel2id:            Relation to unique identifier mapping\n",
    "        self.num_ent:           Number of entities in the Knowledge graph\n",
    "        self.num_rel:           Number of relations in the Knowledge graph\n",
    "        self.embed_dim:         Embedding dimension used\n",
    "        self.data['train']:     Stores the triples corresponding to training dataset\n",
    "        self.data['valid']:     Stores the triples corresponding to validation dataset\n",
    "        self.data['test']:      Stores the triples corresponding to test dataset\n",
    "        self.data_iter:     The dataloader for different data splits\n",
    "\n",
    "        \"\"\"\n",
    "        ent_set, rel_set = OrderedSet(), OrderedSet()\n",
    "        for split in ['train', 'test', 'valid']:\n",
    "            for line in open('./{}/{}/{}.txt'.format(self.data_path, self.data_name,split)):\n",
    "                sub, rel, obj = map(str.lower, line.strip().split('\\t'))\n",
    "                ent_set.add(sub)\n",
    "                rel_set.add(rel)\n",
    "                ent_set.add(obj)\n",
    "\n",
    "        self.ent2id = {ent: idx for idx, ent in enumerate(ent_set)}\n",
    "        self.rel2id = {rel: idx for idx, rel in enumerate(rel_set)}\n",
    "        self.rel2id.update({rel + '_reverse': idx + len(self.rel2id) for idx, rel in enumerate(rel_set)})\n",
    "\n",
    "        self.id2ent = {idx: ent for ent, idx in self.ent2id.items()}\n",
    "        self.id2rel = {idx: rel for rel, idx in self.rel2id.items()}\n",
    "\n",
    "        self.num_ent = len(self.ent2id)\n",
    "        self.num_rel = len(self.rel2id) // 2\n",
    "        print('num_ent {} num_rel {}'.format(self.num_ent, self.num_rel))\n",
    "        \n",
    "        self.triples = ddict(list)\n",
    "\n",
    "        for split in ['train', 'test', 'valid']:\n",
    "            for line in open('./{}/{}/{}.txt'.format(self.data_path,self.data_name, split)):\n",
    "                sub, rel, obj = map(str.lower, line.strip().split('\\t'))\n",
    "                sub, rel, obj = self.ent2id[sub], self.rel2id[rel], self.ent2id[obj]\n",
    "                self.triples[split].append((sub, rel, obj))\n",
    "                self.triples[split].append((obj, rel + self.num_rel, sub))\n",
    "        self.edge_index, self.edge_type = self.construct_adj()\n",
    "\n",
    "    def construct_adj(self):\n",
    "        \"\"\"\n",
    "        Constructor of the runner class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Constructs the adjacency matrix for GCN\n",
    "\n",
    "        \"\"\"\n",
    "        edge_index, edge_type = [], []\n",
    "\n",
    "        for sub, rel, obj in self.triples['train']:\n",
    "            edge_index.append((sub, obj))\n",
    "            edge_type.append(rel)\n",
    "\n",
    "        \n",
    "       \n",
    "        return edge_index, edge_type\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "595e346c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_ent 14541 num_rel 237\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data'\n",
    "#data_name = 'WN18RR'\n",
    "data_name = 'FB15k-237'\n",
    "data = Data(data_path,data_name,device=device)\n",
    "data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be27161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HR2V(torch.nn.Module):\n",
    "    def __init__(self,dim=100):\n",
    "        super(HR2V,self).__init__()\n",
    "        self.linear = torch.nn.Linear(2*dim,dim)\n",
    "        self.ac = torch.nn.ReLU()\n",
    "    def forward(self,h,r):\n",
    "        return h*r\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9841d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HR2V_DeepE(torch.nn.Module):\n",
    "    def __init__(self,dim=100,num_source_layers=1,input_drop=0.4,hidden_drop=0.4,identity_drop=0.0):\n",
    "        super(HR2V_DeepE,self).__init__()\n",
    "        self.input_drop = torch.nn.Dropout(input_drop)\n",
    "        self.input_bn = torch.nn.BatchNorm1d(2*dim)\n",
    "        self.source_layers = torch.nn.Sequential()\n",
    "        \n",
    "        for i in range(num_source_layers):\n",
    "            if i ==0:\n",
    "                input_emb = dim*2\n",
    "            else:\n",
    "                input_emb = dim\n",
    "            self.source_layers.append(DeepEBlock(input_emb,dim,\n",
    "                                                 hidden_drop,torch.nn.ReLU,layers=2,identity_drop=identity_drop))\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self,h,r):        \n",
    "        stacked_inputs = torch.cat([h,r], -1)\n",
    "        x = self.input_bn(stacked_inputs)\n",
    "        x = self.input_drop(x)\n",
    "        \n",
    "        x = self.source_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eea42f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNLayer(MessagePassing):\n",
    "    def __init__(self,edge_index, edge_type,num_ent,hr2v_model,device,data,drop_adj=0.1):\n",
    "        super(GNNLayer,self).__init__(aggr='mean')\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_type = edge_type\n",
    "        self.hr2v_model = hr2v_model\n",
    "        self.data = data\n",
    "        self.drop_adj = drop_adj\n",
    "        self.device = device\n",
    "        self.test_edge_index = torch.LongTensor(edge_index).to(self.device).t()\n",
    "        self.test_edge_type = torch.LongTensor(edge_type).to(self.device)\n",
    "        \n",
    "        self.gen_train_adj(drop_adj = self.drop_adj )\n",
    "        self.max_refresh_adj_num = 50\n",
    "        self.now_adj_num = 0\n",
    "        \n",
    "    def gen_train_adj(self,drop_adj=0.1):\n",
    "        al =list(zip(self.edge_index,self.edge_type))\n",
    "        random.shuffle(al)\n",
    "        leng = int(len(al)*(1-drop_adj))\n",
    "        al = al[:leng]\n",
    "        edge_index,edge_type = zip(*al)\n",
    "        edge_index = torch.LongTensor(edge_index).to(self.device).t()\n",
    "        edge_type = torch.LongTensor(edge_type).to(self.device)\n",
    "        self.train_edge_index =  edge_index\n",
    "        self.train_edge_type = edge_type\n",
    "        #self.train_edge_index =  self.test_edge_index\n",
    "        #self.train_edge_type = self.test_edge_type\n",
    "        \n",
    "        \n",
    "    def forward(self, x, rel_embed,train=False):\n",
    "        if train:\n",
    "            self.now_adj_num += 1\n",
    "            if self.now_adj_num>=self.max_refresh_adj_num:\n",
    "                self.now_adj_num = 0\n",
    "                self.gen_train_adj(drop_adj = self.drop_adj)\n",
    "                \n",
    "        if train:\n",
    "            edge_index = self.train_edge_index\n",
    "            edge_type = self.train_edge_type\n",
    "        else:\n",
    "            edge_index = self.test_edge_index\n",
    "            edge_type = self.test_edge_type\n",
    "            \n",
    "        entity_embed = self.propagate(edge_index, size=None, x=x, edge_type=edge_type, \n",
    "                             rel_embed=rel_embed)\n",
    "        return entity_embed, rel_embed\n",
    "    \n",
    "    def message(self, edge_index_i, edge_index_j, x_i, x_j, edge_type, rel_embed):\n",
    "        rel_embed = torch.index_select(rel_embed, 0, edge_type)\n",
    "        \n",
    "        xj_rel = self.hr2v_model(x_j,rel_embed)\n",
    "        return xj_rel\n",
    "    \n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3f2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self,num_ent,num_rel,init_emb_dim,init_rel_dim,edge_index,edge_type,\n",
    "                 hr2v_model,device='cpu',data=None,pred_model=None,\n",
    "                 num_target_layers=2,target_drop=0.,inner_layers=3,drop_adj=0,beta=0.5,xdrop=0.8):\n",
    "        \n",
    "        super(GNNModel,self).__init__()\n",
    "        self.xdrop=torch.nn.Dropout(xdrop)\n",
    "        self.device = device\n",
    "        self.beta = beta\n",
    "        self.init_ent_emb = self.get_param((num_ent,init_emb_dim))\n",
    "        #默认正反关系，所以加一倍\n",
    "        self.init_rel_emb = self.get_param((num_rel*2,init_rel_dim))\n",
    "        self.edge_index, self.edge_type = edge_index, edge_type\n",
    "        self.hr2v_model = hr2v_model\n",
    "        self.pred_model =pred_model\n",
    "        self.conv_lst = []\n",
    "        self.loss_func = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        #暂时只用1层\n",
    "        conv0 = GNNLayer(self.edge_index, self.edge_type, num_ent,hr2v_model,device=device,data=data,drop_adj=drop_adj)\n",
    "        self.conv_lst.append(conv0)\n",
    "        #self.conv_lst.append(conv1)\n",
    "        \n",
    "        self.data = data\n",
    "        \n",
    "        self.target_layers = torch.nn.Sequential()            \n",
    "        for i in range(num_target_layers):\n",
    "            self.target_layers.append(ResNetBlock(dim,dim,target_drop,torch.nn.ReLU,layers=inner_layers))\n",
    "        self.target_bn = torch.nn.BatchNorm1d(dim)\n",
    "        self.register_parameter('b', Parameter(torch.zeros(num_ent)))\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def get_param(self,shape):\n",
    "        param = Parameter(torch.Tensor(*shape));\n",
    "        xavier_normal_(param.data)\n",
    "        return param\n",
    "    \n",
    "    def forward(self,sub, rel,train=False):\n",
    "        #返回encode过的obj向量、所有实体的向量\n",
    "        sub,rel = self.type_trans(sub,rel)\n",
    "        x,r = self.init_ent_emb,self.init_rel_emb\n",
    "        for conv in self.conv_lst:\n",
    "            x,r = conv(x,r,train)\n",
    "        \n",
    "        \n",
    "        \n",
    "        dist2 = torch.nn.functional.pairwise_distance(x, self.init_ent_emb, p=2)\n",
    "        l2_loss = torch.mean(dist2)\n",
    "        \n",
    "        #x=self.xdrop(x)\n",
    "        \n",
    "        x =   x + self.init_ent_emb\n",
    "        \n",
    "        sub_emb = torch.index_select(self.init_ent_emb, 0, sub)\n",
    "        rel_emb = torch.index_select(self.init_rel_emb, 0, rel)\n",
    "        \n",
    "        enc_vec = self.pred_model(sub_emb, rel_emb)\n",
    "        \n",
    "        all_ent_vec = self.target_bn(x)\n",
    "        all_ent_vec = self.target_layers(all_ent_vec)\n",
    "        \n",
    "        \n",
    "        scores = torch.mm(enc_vec,all_ent_vec.transpose(1,0))\n",
    "        #scores += self.b.expand_as(scores)\n",
    "        \n",
    "        \n",
    "        return scores,l2_loss\n",
    "    \n",
    "    def forward_and_loss(self,sub,rel,obj):\n",
    "        sub,rel,obj = self.type_trans(sub,rel,obj)\n",
    "        scores,l2_loss = self.forward(sub,rel,train=True)\n",
    "        return self.loss_func(scores,obj.long()) + l2_loss * self.beta\n",
    "    \n",
    "    \n",
    "    def type_trans(self,*args):\n",
    "        trans = []\n",
    "        for arg in args:\n",
    "            if isinstance(arg,np.ndarray):\n",
    "                trans.append(torch.from_numpy(arg).to(self.device))\n",
    "            elif isinstance(arg,list) or isinstance(arg,tuple):\n",
    "                trans.append(torch.tensor(arg).to(self.device))\n",
    "            elif isinstance(arg,torch.Tensor):\n",
    "                trans.append(arg.to(self.device))\n",
    "            else:\n",
    "                print('type error')\n",
    "                raise \n",
    "        return trans\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2f452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(triples,batch_size=2000):\n",
    "    np.random.shuffle(triples)\n",
    "    hs,rs,ts = zip(*triples)\n",
    "    num_batches = len(triples) // batch_size + 1\n",
    "    for i in range(num_batches):\n",
    "        yield  np.array(hs[batch_size*i:batch_size*(i+1)]),np.array(rs[batch_size*i:batch_size*(i+1)]),\\\n",
    "        np.array(ts[batch_size*i:batch_size*(i+1)])\n",
    "    \n",
    "target_dict = get_target_dict(data.triples['train'],data.triples['valid'],data.triples['test'])\n",
    "def evaluate(model,x_test,batch_size,target_dict):\n",
    "    #target_dict:用于filter\n",
    "    len_test = len(x_test)    \n",
    "    batch_num = math.ceil(len(x_test) / batch_size)\n",
    "    tail_scores_all = []\n",
    "    tail_label = []\n",
    "    \n",
    "    for i in range(batch_num):\n",
    "        batch_data  = x_test[batch_size*i:batch_size*(i+1)]\n",
    "        batch_h,batch_r,batch_t = batch_data[:,0],batch_data[:,1],batch_data[:,2]\n",
    "        tail_scores,_  = model.forward(batch_h,batch_r,train=False)\n",
    "        tail_scores = tail_scores.cpu().detach().numpy()\n",
    "        tail_scores_all.append(tail_scores)\n",
    "        tail_label.append(batch_t)\n",
    "        \n",
    "    \n",
    "    tail_scores_all = np.concatenate(tail_scores_all,axis=0)  \n",
    "    tail_label = np.concatenate(tail_label,axis=0)  \n",
    "    \n",
    "    def cal_result(scores,labels,x_test,target_dict):\n",
    "        ranks = []\n",
    "        for i in range(len(labels)):\n",
    "            arr = scores[i]\n",
    "            mark = labels[i]\n",
    "            h,r,t = x_test[i]\n",
    "            mark_value = arr[mark]\n",
    "            \n",
    "            ##filter\n",
    "            targets = target_dict[(h,r)]\n",
    "            for target in targets:\n",
    "                if target != mark:\n",
    "                    arr[target] = np.finfo(np.float32).min\n",
    "            ##\n",
    "            rank = np.sum(arr>=mark_value)\n",
    "            #rank+=1\n",
    "            ranks.append(rank)\n",
    "            \n",
    "        mr, mrr, hits1, hits10 =0,[],[],[]\n",
    "        mr = np.average(ranks)\n",
    "        \n",
    "        for rank in ranks:\n",
    "            mrr.append(1/rank)\n",
    "            if rank == 1:\n",
    "                hits1.append(1)\n",
    "            else:\n",
    "                hits1.append(0)\n",
    "            if rank <= 10:\n",
    "                hits10.append(1)\n",
    "            else:\n",
    "                hits10.append(0)\n",
    "        mrr = np.average(mrr)\n",
    "        hits1 = np.average(hits1)\n",
    "        hits10 = np.average(hits10)\n",
    "        result = {'mr':mr, 'mrr':mrr, 'hits1':hits1, 'hits10':hits10}\n",
    "        return result    \n",
    "    \n",
    "    tail_result = cal_result(tail_scores_all,tail_label, x_test,target_dict)\n",
    "    return {'mr':tail_result['mr'], 'mrr':tail_result['mrr'], \n",
    "            'hits@1':tail_result['hits1'], 'hits@10':tail_result['hits10']}\n",
    "\n",
    "def get_target_dict(train_doubles,x_valid,x_test):\n",
    "    target_dict = {}\n",
    "    for h,r,t in train_doubles:\n",
    "        if (h,r) not in target_dict:\n",
    "            target_dict[(h,r)] = set()\n",
    "        target_dict[(h,r)].add(t)\n",
    "    for h,r,t in x_valid:\n",
    "        if (h,r) not in target_dict:\n",
    "            target_dict[(h,r)] = set()\n",
    "        target_dict[(h,r)].add(t)\n",
    "    for h,r,t in x_test:\n",
    "        if (h,r) not in target_dict:\n",
    "            target_dict[(h,r)] = set()\n",
    "        target_dict[(h,r)].add(t) \n",
    "    return target_dict \n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self,model,data,batch_size=1000,lr=0.003,weight_decay=5e-4 ,min_lr=1e-7,patience=5,factor=0.5):\n",
    "        self.model = model\n",
    "        self.opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.opt, 'min',\n",
    "                                                                    factor=factor,min_lr=min_lr,patience=patience,verbose=True)\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def train(self,max_epoch=200):\n",
    "        best_mrr = 0.0\n",
    "        for epoch in range(max_epoch):\n",
    "            if epoch>500:\n",
    "                self.batch_size = 500\n",
    "            self.train_one_epoch(epoch=epoch)\n",
    "            if epoch!=0 and epoch %10 ==0:\n",
    "                results = self.evaluate()\n",
    "                print('previous best mrr',best_mrr)\n",
    "                if results['mrr']>best_mrr:\n",
    "                    print('new best mrr',results['mrr'])\n",
    "                    best_mrr = results['mrr']\n",
    "                    torch.save(self.model,'fb_rgnn.pt')\n",
    "                \n",
    "                print(results)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        return evaluate(self.model,np.array(self.data.triples['valid']),batch_size=self.batch_size,target_dict=target_dict)\n",
    "        \n",
    "    def train_one_epoch(self,epoch=None):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        train_triples = self.data.triples['train']\n",
    "        \n",
    "        for i,(hs,rs,ts) in enumerate(data_generator(train_triples,self.batch_size)):\n",
    "            self.opt.zero_grad()\n",
    "            loss = self.model.forward_and_loss(hs,rs,ts)\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "        print('epoch:',epoch,'train loss:',np.average(losses))\n",
    "        self.scheduler.step(np.average(losses))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6f36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5459ea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta = 0.5\n",
    "drop_adj = 0.15\n",
    "\n",
    "dim = 300\n",
    "input_drop = 0.4\n",
    "hidden_drop = 0.4\n",
    "identity_drop = 0.01\n",
    "num_source_layers =40\n",
    "num_target_layers=1\n",
    "target_drop=0.4\n",
    "inner_layers=3\n",
    "weight_decay = 5e-8\n",
    "lr = 0.003\n",
    "batch_size = 1000\n",
    "hr2v_model = HR2V(dim=dim)\n",
    "pred_model = HR2V_DeepE(dim=dim,num_source_layers=num_source_layers,input_drop=input_drop,\n",
    "                        hidden_drop=hidden_drop,identity_drop=identity_drop)\n",
    "\n",
    "gnn_model = GNNModel(data.num_ent,data.num_rel,dim,dim,data.edge_index,data.edge_type,\n",
    "                     hr2v_model,device=device,data=data,pred_model=pred_model,\n",
    "                    num_target_layers=num_target_layers,target_drop=target_drop,\n",
    "                     inner_layers=inner_layers,drop_adj=drop_adj,beta=beta,xdrop=0)\n",
    "\n",
    "\n",
    "trainer = Trainer(gnn_model,data,batch_size=batch_size,weight_decay = weight_decay,lr = lr)\n",
    "\n",
    "trainer.train(max_epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06683eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
